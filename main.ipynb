{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instagram Like Prediction @310ai Competition\n",
    "\n",
    "This notebook is for the competition posted by the @310ai on 15th of April. I will approach the competition as a project following the CRISP-DM methodology and try to explain the approach in every steps of the way.\n",
    "\n",
    "The main and short summary of this competition is **\"given an Instagram post predict the number of likes\"**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "First thing first, there are some important points that we have to consider which are forced by the Instagram. This points will result in some features that are effective in percision of the model. In the following section we will discuss them further.\n",
    "\n",
    "***Are we try to predict the number of likes for an Instagram post of our own or not?***\n",
    "\n",
    "This question might seem a little odd, but let me explain it. Each Instagram post consists of some metrics that show the performance of the post among the users. We will call these **\"Performance Metrics\"**. Some of these performance metrics such as amount of like, amount of columns, caption and etc, are publicly availble, in other words, any user on the Instagram can see them.\n",
    "\n",
    "But some of the performance metrics, are not publicly available, in order to see them, we need to authenticate as the owner of the page (will discuss about this part further in this section.), some of these private performance metrics are, amount of share, amount of save, amount of reach, amount of profile visits, amount of follows, amount of impression and etc.\n",
    "\n",
    "Obviously, if we try to predict the amount of like for a page that we don't own, we can not access these features, we will go for a page that we don't have access to it for this competition.\n",
    "\n",
    "Another to have in mind is that, since the post we are going to predict the amount of like for it, is not actually existing, the amount of performance metrics can't be predicted preciesly. In other words, how we can estimate the amount of comments a hypothetical post might recieve if we don't post it actually. Due to this abstraction, the performance metrics for each post is not a good feature for this deed.\n",
    "\n",
    "In the further section I will try to address the questions of the competitions in combination of code and text. Please have in mind to follow the chosen methodology I might change the order of questions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Requirements and Data Collection\n",
    "\n",
    "In this section I will tackle the questions mainly related to these parts of the challenge. As we discussed above some useful features introduced that might have effect on the precision of the prediction. But there are some other features, further I will point to some features that are related to the page of the published post.\n",
    "\n",
    "### What Features you used?\n",
    "\n",
    "Each and every page on the Instagram has some features that will distinguish it from other pages, some of these features are like the features discussed above, performance metrics, and some of them are identifiers. Some of the identifiers features are:\n",
    "- `id`: a unique id that is allocated by the Instagram.\n",
    "- `username`: a unique username that each user when created the page chose.\n",
    "\n",
    "Also there are some other features that we will investigate, these features are:\n",
    "- `category_name`: each page based on the published content and some other traits, are categorized into different categories, for instance, Blogger, Personal Blog, Design & Fashion, chef and etc.\n",
    "- `follower`: amount of followers the page has.\n",
    "- `following`: amount of pages that the target page is following.\n",
    "- `ar_effect`: whether the page has published ar effects in the Instagram or not.\n",
    "- `type_business`: whether the page identified itself as business account or not.\n",
    "- `type_professional`: whether the page identified itself as professional account or not.\n",
    "- `verified`: whether the page is verified or not.\n",
    "- `reel_count`: amount of igtvs posted by the page.\n",
    "- `media_count`: amount of posts, posted by the page.\n",
    "\n",
    "There are some features that are collected organically but can be calculated in the process of feature engineering. Some of them are:\n",
    "- `reel_view`: The average view of igtvs posted by the page.\n",
    "- `reel_comment`: The average of comments igtvs acquired.\n",
    "- `reel_like`: The average of likes igtvs got.\n",
    "- `reel_duration`: The average of igtv's duration posted by the page.\n",
    "- `reel_frequency`: How often the page have posted the reels.\n",
    "- `media_avg_view`: The average view of media posted by the page.\n",
    "- `media_avg_comment`: The average of comments media acquired.\n",
    "- `media_avg_like`: The average of likes media got.\n",
    "- `media_avg_duration`: The average of media's duration posted by the page.\n",
    "- `media_frequency`: How often the page have posted the media.\n",
    "\n",
    "Last but not least, is the content of the image itself. There are multiple ways to have the content of the image as feature. For instance we can have a classifier network to detect what objects are present in the image and pass them to the like predictor model. Other heuristic approaches might result in a good model, such as passing the image vector generated by the last hidden layer of a classification network as a standalone feature.\n",
    "\n",
    "As you are aware, choosing the best strategy requires some tests, such as A/B tests and trial and error ones, for now I will chose the strategy which will be discussed further that is fastest and heuristic.\n",
    "\n",
    "It's been some time that the Meta, is using an object detection model for generating the Alt Text attribute for the posts. Due to the resources the Meta have in its disposal, this model is extremly face and reliable since it is ran on the server side. Thus for this approach we will use the result of the what objects are present in the image as feature."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we collect the data?\n",
    "\n",
    "As we discussed above, there are different kind of features, and each group can be collected via different methods.\n",
    "\n",
    "The Instagram provides an API for developers, but due some restrictions and limitations, this API can not provide us the data that we seek. Based on this facts, we will use a heuristic way to collect the data. There will be 2 approaches regarding the matter. one approach which is not very tech-friendly (:D) is to create a scrapper with Selenium page in python to scrap the information we need. Selenium is a website testing library in python that can also be utilized into a webscrapper. This approach has another limitation excluse for users like me, since I'm in Iran right now, access to the Instagram is restricted and we have to use VPNs and geo-restriction bypasses, these tools add another layer of challenge and additional bottleneck. Another approach that I try to utilize, is to use the graphql endpoints to recieve the information needed in JSON format. Eventhough still use of VPNs and similar tools is needed in this approach, but unlike the Selenium this approach doesn't require to load the GUI of Instagram, its much more faster and eligble in a pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- end point for user information:\n",
    "`https://www.instagram.com/{username}/?__a=1&__d=dis\n",
    "`\n",
    "\n",
    "- end point for post information:\n",
    "`https://www.instagram.com/p/{post_ID}/?__a=1&__d=dis\n",
    "`\n",
    "\n",
    "getting training data for the model:\n",
    "- each json response of an account gives 12 latest post\n",
    "information:\n",
    "\n",
    "  - Alt text information is here: `data['graphql']['user']['edge_owner_to_timeline_media']['edges'][0]['node']['accessibility_caption']`\n",
    "    - each node has type, `GraphImage` is posts which have alt text.\n",
    "    - `GraphVideo` doesn't have alt text.\n",
    "    - `GraphSideCar` is carousel and have alt text.\n",
    "  - number of comments is here: `data['graphql']['user']['edge_owner_to_timeline_media']['edges'][0]['node']['edge_media_to_comment']['count']`\n",
    "  - number of likes is here: `data['graphql']['user']['edge_owner_to_timeline_media']['edges'][0]['node']['edge_liked_by']['count']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# reading credentials for loging into the instagram account\n",
    "with open('credentials.json') as f:\n",
    "    creds = json.load(f)\n",
    "    login_username = creds['username']\n",
    "    login_password = creds['password']\n",
    "\n",
    "# reading accounts lists for gathering training data.\n",
    "with open('Data/top_100_follower.txt') as f:\n",
    "    lines = f.readlines()\n",
    "top_100_followers = lines[0].split(',')\n",
    "\n",
    "with open('Data/top_100_posts.txt') as f:\n",
    "    lines = f.readlines()\n",
    "top_100_posts = lines[0].split(',')\n",
    "\n",
    "# since added try exception in the main body of collecting data, this section is probably unnecessary, double check it.\n",
    "main_accounts_df = pd.DataFrame(columns=['id', 'username', 'category_name', 'follower', 'following', 'ar_effect', 'type_business', 'type_professional', 'verified', 'reel_count', 'reel_avg_view', 'reel_avg_comment', 'reel_avg_like', 'reel_avg_duration', 'reel_frequency', 'media_count', 'media_avg_comment', 'media_avg_like', 'media_frequency'])\n",
    "main_posts_df = pd.DataFrame(columns=['shortcode', 'post_type', 'username', 'like', 'comment', 'object_1', 'object_2', 'object_3', 'object_4', 'object_5','object_6'])\n",
    "\n",
    "def flatten(lst):\n",
    "    \"\"\"A helper function to flatten any dimensional python list to 1D one.\n",
    "\n",
    "    Args:\n",
    "        lst (list): multi dimension python list\n",
    "\n",
    "    Returns:\n",
    "        list: flattened list\n",
    "    \"\"\"\n",
    "    rt = []\n",
    "    for i in lst:\n",
    "        if isinstance(i,list): rt.extend(flatten(i))\n",
    "        else: rt.append(i)\n",
    "    return rt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging into the Instagram account\n",
    "This step is necesary for getting information of the images, since majority of information in Instagram are locked behind the authentication wall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login successful\n",
      "csrf_token:  1xrrX3tES6MlqAq5x7MAha1KEiS3gZu1\n",
      "session_id:  1691538713%3AUbWBVeVU6JWjb6%3A18%3AAYeKPcFiSjUVnxlz0dDQWoJ0biKlvNjyQWLsLXZ4PA\n"
     ]
    }
   ],
   "source": [
    "link = 'https://www.instagram.com/accounts/login/'\n",
    "login_url = 'https://www.instagram.com/accounts/login/ajax/'\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36',\n",
    "            'referer':'https://www.instagram.com/',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'none',\n",
    "            'Sec-Fetch-User': '?1',\n",
    "            'TE': 'trailers'\n",
    "}\n",
    "\n",
    "\n",
    "current_time = int(datetime.now().timestamp())\n",
    "response = requests.Session().get(link, headers=headers)\n",
    "if response.ok:\n",
    "    csrf = re.findall(r'csrf_token\\\\\":\\\\\"(.*?)\\\\\"',response.text)[0]\n",
    "    username = login_username\n",
    "    password = login_password\n",
    "\n",
    "    payload = {\n",
    "        'username': username,\n",
    "        'enc_password': f'#PWD_INSTAGRAM_BROWSER:0:{current_time}:{password}',\n",
    "        'queryParams': {},\n",
    "        'optIntoOneTap': 'false',\n",
    "        'stopDeletionNonce': '',\n",
    "        'trustedDeviceRecords': '{}'\n",
    "    }\n",
    "\n",
    "    login_header = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36',\n",
    "        \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "        \"Referer\": \"https://www.instagram.com/accounts/login/\",\n",
    "        \"X-CSRFToken\": csrf,\n",
    "        'Accept': '*/*',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'X-Instagram-AJAX': 'c6412f1b1b7b',\n",
    "        'X-IG-App-ID': '936619743392459',\n",
    "        'X-ASBD-ID': '198387',\n",
    "        'X-IG-WWW-Claim': '0',\n",
    "        'X-Requested-With': 'XMLHttpRequest',\n",
    "        'Origin': 'https://www.instagram.com',\n",
    "        'DNT': '1',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Referer': 'https://www.instagram.com/accounts/login/?',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "    }\n",
    "\n",
    "    login_response = requests.post(login_url, data=payload, headers=login_header)\n",
    "    json_data = json.loads(login_response.text)\n",
    "\n",
    "\n",
    "    if json_data['status'] == 'fail':\n",
    "        print(json_data['message'])\n",
    "\n",
    "    elif json_data[\"authenticated\"]:\n",
    "        print(\"login successful\")\n",
    "        cookies = login_response.cookies\n",
    "        cookie_jar = cookies.get_dict()\n",
    "        csrf_token = cookie_jar['csrftoken']\n",
    "        print(\"csrf_token: \", csrf_token)\n",
    "        session_id = cookie_jar['sessionid']\n",
    "        print(\"session_id: \", session_id)\n",
    "\n",
    "    else:\n",
    "        print(\"login failed \", login_response.text)\n",
    "else:\n",
    "    print('error')\n",
    "    print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell is the main cell for collecting the data from the Instagram, since this code block is the longest block in the workspace, it's worth to discuss the parts of its structure. Please have in mind the best design pattern for this kind of task, is to create pipeline, but since this is a competition and understanding a pipeline might be difficult for reviewrs, I stick with this approach regarding the matter.\n",
    "\n",
    "First thing, I have to check whether the data is present or not, if the **accounts** and **posts** dataset are present I'm reading them, otherwise I'm creating empty dataframes for each one of them with their corresponding features. I have to read the names of the accounts I want to get their information, for the training of this model, I have selected the top 100 pages with the most followers and top 100 pages with the most published posts. I call these **accounts dataset**.\n",
    "\n",
    "For each username in the accounts dataset, I do these procedures:\n",
    "1. I check whether I had acquired that account information or no, if I had, skip that account and go to the next account.\n",
    "2. Then I send a request containing appropriate headers and previously acquired cookies from logining into the Instagram, to recieve account information. I sanity check the response to validate whether we have got the correct response or it's faulty (i.e. empty response, page got private, etc.).\n",
    "3. Previously Discussed features then are extracted from the response json and saved into their coresponding variables or lists, some of these features have to be calculated, for instance, media & reel frequency, view, like, comment, duration average and etc. These features are calculated and saved into their correspoding variables.\n",
    "4. I create a temporary dataframe for each account and add it to the main accounts dataframe.\n",
    "5. Almost the same procedure is done for the posts information.\n",
    "6. In the end, we will have 5 seconds delay between each username process, to honoring the rate limit of the Instagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: theshaderoom\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: hollywoodunlocked\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: rvcjinsta\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: manotoofficial\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: tvnnoticias\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: instablog9ja\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: worldstar\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: uae_barq\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: telemetro\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: bellanaijaonline\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: fashionnova\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: 3meed_news\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: melodia_musik\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: aboutcirebonid\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: infodenpasar\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: radiofarda\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: instantbollywood\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: iranintltv\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: fashionbombdaily\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: eldiario\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: arab2turk\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: albayannews\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: vladtv\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: coppamagz\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: crictracker\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: nba\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: snoopdogg\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: dagelan\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: spiritualword\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: memelasdeorizaba\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: cabronazi\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: deertybhr\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: nfl\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: viralbhayani\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: complex\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: vengalaalegriatva\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [00:00<00:01, 36.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response is empty for vengalaalegriatva skipping...\n",
      "Getting Account Information: zonakorea\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: bleacherreport\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: laliga\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: tommyphillipsiv\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: officialmnctv\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: sportscenter\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: jdemsey\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: theybf_daily\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: idntimes\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: foxnews\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: diariodetransferenciasdt\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: hespress\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: koreadispatch\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: vicidolls\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: wwe\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: emaratalyoum\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: sonytvofficial\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: ayahlyfans\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: espn\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: keaw_jung\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: djkhaled\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: rap\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: alkhaleej.ae\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: colorstv\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: khaleejtimes\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: thetinderblog\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: jktinfo\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: routineofnepalbanda\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: alroeya\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: icc\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: laiguanatv\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: detikcom\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: theneighborhoodtalk\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: entre.nous.official\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: nocap\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: lahamag\n",
      "Adding lahamag information...\n",
      "Getting Posts Information: lahamag\n",
      "Adding lahamag posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [00:11<00:05,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: flamengo\n",
      "Adding flamengo information...\n",
      "Getting Posts Information: flamengo\n",
      "Adding flamengo posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [00:21<00:11,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: eye.on.palestine\n",
      "Adding eye.on.palestine information...\n",
      "Getting Posts Information: eye.on.palestine\n",
      "Adding eye.on.palestine posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [00:32<00:19,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: samuelsianto\n",
      "Adding samuelsianto information...\n",
      "Getting Posts Information: samuelsianto\n",
      "Adding samuelsianto posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [00:40<00:26,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: elchiringuitotv\n",
      "Adding elchiringuitotv information...\n",
      "Getting Posts Information: elchiringuitotv\n",
      "Adding elchiringuitotv posts information...\n",
      "Waiting 5 seconds...\n",
      "Getting Account Information: abnajnews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [00:56<00:26,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding abnajnews information...\n",
      "Getting Posts Information: abnajnews\n",
      "Adding abnajnews posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [01:02<00:49,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: sayidatynet\n",
      "Adding sayidatynet information...\n",
      "Getting Posts Information: sayidatynet\n",
      "Adding sayidatynet posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [01:13<01:01,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: 9gag\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: deals_with_angel\n",
      "Adding deals_with_angel information...\n",
      "Getting Posts Information: deals_with_angel\n",
      "Adding deals_with_angel posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [01:20<00:58,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: titoloshop\n",
      "Adding titoloshop information...\n",
      "Getting Posts Information: titoloshop\n",
      "Adding titoloshop posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [01:28<01:05,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: saboskirt\n",
      "Adding saboskirt information...\n",
      "Getting Posts Information: saboskirt\n",
      "Adding saboskirt posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [01:36<01:12,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: ufc\n",
      "Adding ufc information...\n",
      "Getting Posts Information: ufc\n",
      "Adding ufc posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [01:46<01:25,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: onepalon\n",
      "Adding onepalon information...\n",
      "Getting Posts Information: onepalon\n",
      "Adding onepalon posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [01:55<01:32,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: berrics\n",
      "Adding berrics information...\n",
      "Getting Posts Information: berrics\n",
      "Adding berrics posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [02:05<01:41,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: yallneedbutera\n",
      "Adding yallneedbutera information...\n",
      "Getting Posts Information: yallneedbutera\n",
      "Adding yallneedbutera posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [02:15<01:42,  7.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: indiansuperleague\n",
      "Adding indiansuperleague information...\n",
      "Getting Posts Information: indiansuperleague\n",
      "Adding indiansuperleague posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [02:25<01:46,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: nails_masters\n",
      "Adding nails_masters information...\n",
      "Getting Posts Information: nails_masters\n",
      "Adding nails_masters posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88/100 [02:34<01:39,  8.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: okezonecom\n",
      "Adding okezonecom information...\n",
      "Getting Posts Information: okezonecom\n",
      "Adding okezonecom posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [02:42<01:30,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: chouftv_official\n",
      "Adding chouftv_official information...\n",
      "Getting Posts Information: chouftv_official\n",
      "Adding chouftv_official posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [02:53<01:30,  9.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: gazetemagazin\n",
      "Adding gazetemagazin information...\n",
      "Getting Posts Information: gazetemagazin\n",
      "Adding gazetemagazin posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [03:02<01:22,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: sportskeedacricket\n",
      "Adding sportskeedacricket information...\n",
      "Getting Posts Information: sportskeedacricket\n",
      "Adding sportskeedacricket posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [03:10<01:09,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: zeetv\n",
      "Adding zeetv information...\n",
      "Getting Posts Information: zeetv\n",
      "Adding zeetv posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [03:22<01:06,  9.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: corinthians\n",
      "Adding corinthians information...\n",
      "Getting Posts Information: corinthians\n",
      "Adding corinthians posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [03:36<01:05, 10.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: natgeo\n",
      "User information already exist, skipping...\n",
      "Getting Account Information: jokezar\n",
      "Adding jokezar information...\n",
      "Getting Posts Information: jokezar\n",
      "Adding jokezar posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [03:43<00:30,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: ultimahoracol\n",
      "Adding ultimahoracol information...\n",
      "Getting Posts Information: ultimahoracol\n",
      "Adding ultimahoracol posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [03:51<00:22,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: sosmedmakassar\n",
      "Adding sosmedmakassar information...\n",
      "Getting Posts Information: sosmedmakassar\n",
      "Adding sosmedmakassar posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [04:02<00:17,  8.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: indonesiabertauhidofficial\n",
      "Adding indonesiabertauhidofficial information...\n",
      "Getting Posts Information: indonesiabertauhidofficial\n",
      "Adding indonesiabertauhidofficial posts information...\n",
      "Waiting 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [04:11<00:08,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Account Information: eddiempr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:12<00:00,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response is empty for eddiempr skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    main_accounts_df = pd.read_csv('Data/accounts.csv')\n",
    "    main_posts_df = pd.read_csv('Data/posts.csv')\n",
    "    main_accounts_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    main_posts_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "except:\n",
    "    main_accounts_df = pd.DataFrame(columns=['id', 'username', 'category_name', 'follower', 'following', 'ar_effect', 'type_business', 'type_professional', 'verified', 'reel_count', 'reel_avg_view', 'reel_avg_comment', 'reel_avg_like', 'reel_avg_duration', 'reel_frequency', 'media_count', 'media_avg_comment', 'media_avg_like', 'media_frequency'])\n",
    "    main_posts_df = pd.DataFrame(columns=['shortcode', 'post_type', 'username', 'like', 'comment', 'object_1', 'object_2', 'object_3', 'object_4', 'object_5','object_6'])\n",
    "\n",
    "\n",
    "for username in tqdm(top_100_followers + top_100_posts):\n",
    "    print(f'Getting Account Information: {username}')\n",
    "    if main_accounts_df['username'].str.contains(f'{username}').any():\n",
    "        print('User information already exist, skipping...')\n",
    "        continue\n",
    "    if main_posts_df['username'].str.contains(f'{username}').any():\n",
    "        print('User post information already exist, skiping...')\n",
    "        continue\n",
    "    # loading account information\n",
    "    session = {\n",
    "            \"csrf_token\": csrf_token,\n",
    "            \"session_id\": session_id\n",
    "        }\n",
    "\n",
    "    headers = {\n",
    "            \"x-csrftoken\": session['csrf_token'],\n",
    "            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36',\n",
    "            \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "            \"Referer\": \"https://www.instagram.com/accounts/login/\",\n",
    "            'Accept': '*/*',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'X-Instagram-AJAX': 'c6412f1b1b7b',\n",
    "            'X-IG-App-ID': '936619743392459',\n",
    "            'X-ASBD-ID': '198387',\n",
    "            'X-IG-WWW-Claim': '0',\n",
    "            'X-Requested-With': 'XMLHttpRequest',\n",
    "            'Origin': 'https://www.instagram.com',\n",
    "            'DNT': '1',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Referer': 'https://www.instagram.com/accounts/login/?',\n",
    "            'Sec-Fetch-Dest': 'empty',\n",
    "            'Sec-Fetch-Mode': 'cors',\n",
    "            'Sec-Fetch-Site': 'same-origin',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'none',\n",
    "            'Sec-Fetch-User': '?1',\n",
    "            'TE': 'trailers'\n",
    "        }\n",
    "\n",
    "    cookies = {\n",
    "            \"sessionid\": session['session_id'],\n",
    "            \"csrftoken\": session['csrf_token']\n",
    "        }\n",
    "    url = f'https://www.instagram.com/{username}/?__a=1&__d=dis'\n",
    "    res = requests.get(url, headers=headers, cookies=cookies)\n",
    "    # add error handling here based on response codes, reference -> InstagramBot.py\n",
    "\n",
    "    data = res.json()\n",
    "    if not data:\n",
    "        print(f'Response is empty for {username} skipping...')\n",
    "        continue\n",
    "    followers = data['graphql']['user']['edge_followed_by']['count']\n",
    "    following = data['graphql']['user']['edge_follow']['count']\n",
    "    ar_effect = data['graphql']['user']['has_ar_effects']\n",
    "    id = data['graphql']['user']['id']\n",
    "    type_business = data['graphql']['user']['is_business_account']\n",
    "    type_professional = data['graphql']['user']['is_professional_account']\n",
    "    category = data['graphql']['user']['category_name']\n",
    "    verified = data['graphql']['user']['is_verified']\n",
    "    reel_count = data['graphql']['user']['edge_felix_video_timeline']['count']\n",
    "    media_count = data['graphql']['user']['edge_owner_to_timeline_media']['count']\n",
    "    username = data['graphql']['user']['username']\n",
    "\n",
    "    reel_view_list = []\n",
    "    reel_like_list = []\n",
    "    reel_comment_list = []\n",
    "    reel_duration_list = []\n",
    "    reel_timestamp_list = []\n",
    "\n",
    "    media_like_list = []\n",
    "    media_comment_list = []\n",
    "    media_timestamp_list = []\n",
    "\n",
    "    for video in data['graphql']['user']['edge_felix_video_timeline']['edges']:\n",
    "        reel_view_list.append(video['node']['video_view_count'])\n",
    "        reel_comment_list.append(video['node']['edge_media_to_comment']['count'])\n",
    "        reel_timestamp_list.append(video['node']['taken_at_timestamp'])\n",
    "        reel_like_list.append(video['node']['edge_liked_by']['count'])\n",
    "        reel_duration_list.append(video['node']['video_duration'])\n",
    "    \n",
    "    # sometimes instagram result for video duration is None, this is sanity check\n",
    "    reel_duration_list = [0 if duration is None else duration for duration in reel_duration_list]\n",
    "\n",
    "    for medium in data['graphql']['user']['edge_owner_to_timeline_media']['edges']:\n",
    "        media_like_list.append(medium['node']['edge_liked_by']['count'])\n",
    "        media_comment_list.append(medium['node']['edge_media_to_comment']['count'])\n",
    "        media_timestamp_list.append(medium['node']['taken_at_timestamp'])\n",
    "\n",
    "    \n",
    "    reel_utc_list = [datetime.utcfromtimestamp(ts) for ts in reel_timestamp_list]\n",
    "    media_utc_list = [datetime.utcfromtimestamp(ts) for ts in media_timestamp_list]\n",
    "\n",
    "    reel_utc_difference_list = [reel_utc_list[i] - reel_utc_list[i+1] for i in range(len(reel_utc_list) - 1)]\n",
    "    media_utc_difference_list = [media_utc_list[i] - media_utc_list[i+1] for i in range(len(media_utc_list) - 1)]\n",
    "\n",
    "    if reel_count > 1:\n",
    "        reel_frequency = np.mean(reel_utc_difference_list).days + (np.mean(reel_utc_difference_list).seconds / 86_400) + (np.mean(reel_utc_difference_list).microseconds / 1_000_000 / 84_600)\n",
    "    else:\n",
    "        reel_frequency = 0\n",
    "    media_frequency = np.mean(media_utc_difference_list).days + (np.mean(media_utc_difference_list).seconds / 86_400) + (np.mean(media_utc_difference_list).microseconds / 1_000_000 / 84_600)\n",
    "\n",
    "    reel_view_mean = np.mean(reel_view_list)\n",
    "    reel_like_mean = np.mean(reel_like_list)\n",
    "    reel_comment_mean = np.mean(reel_comment_list)\n",
    "    reel_duration_mean = np.mean(reel_duration_list)\n",
    "\n",
    "    media_like_mean = np.mean(media_like_list)\n",
    "    media_comment_mean = np.mean(media_comment_list)\n",
    "\n",
    "    entry_lst = [id, username, category, followers, following, ar_effect, type_business, type_professional, verified, reel_count, reel_view_mean, reel_comment_mean, reel_like_mean, reel_duration_mean, reel_frequency, media_count, media_comment_mean, media_like_mean, media_frequency]\n",
    "\n",
    "    account_df = pd.DataFrame() #reset variable\n",
    "    account_df = pd.DataFrame([entry_lst], columns=['id', 'username', 'category_name', 'follower', 'following', 'ar_effect', 'type_business', 'type_professional', 'verified', 'reel_count', 'reel_avg_view', 'reel_avg_comment', 'reel_avg_like', 'reel_avg_duration', 'reel_frequency', 'media_count', 'media_avg_comment', 'media_avg_like', 'media_frequency'])\n",
    "\n",
    "    if account_df.username.isin(main_accounts_df.username).bool():\n",
    "        print('User information already exist, skipping...')\n",
    "        continue\n",
    "    else:\n",
    "        print(f'Adding {username} information...')\n",
    "        account_df = account_df.astype({\n",
    "            'ar_effect': bool,\n",
    "            'type_business': bool,\n",
    "            'type_professional': bool,\n",
    "            'verified': bool,\n",
    "        })\n",
    "        main_accounts_df = pd.concat([main_accounts_df, account_df], axis=0, join='outer')\n",
    "    \n",
    "    # adding user's posts information\n",
    "    print(f'Getting Posts Information: {username}')\n",
    "    # main lists structure is:\n",
    "    # shortcode, post_type, username, objects\n",
    "    posts_lst = []\n",
    "    for post in data['graphql']['user']['edge_owner_to_timeline_media']['edges']:\n",
    "        temp_lst = []\n",
    "        objects = []\n",
    "        temp_lst.append(post['node']['shortcode'])\n",
    "        temp_lst.append(post['node']['__typename'])\n",
    "        temp_lst.append(data['graphql']['user']['username'])\n",
    "        temp_lst.append(post['node']['edge_liked_by']['count'])\n",
    "        temp_lst.append(post['node']['edge_media_to_comment']['count'])\n",
    "        if post['node']['__typename'] == 'GraphImage' or post['node']['__typename'] == 'GraphSidecar':\n",
    "            if post['node']['accessibility_caption'] == None:\n",
    "                objects = []\n",
    "                continue\n",
    "            # split object-detection output\n",
    "            try:\n",
    "                objects = post['node']['accessibility_caption'].split('.')[1]\n",
    "            except:\n",
    "                continue            \n",
    "            # terminating empty lists\n",
    "            if objects:\n",
    "                try:\n",
    "                    # cutting objects\n",
    "                    objects = objects.split('of')[1]\n",
    "                    objects = objects.split('and', 1)\n",
    "                    objects[0] = objects[0].split(',')\n",
    "                    if 'text' in objects[1]:\n",
    "                        objects[1] = 'text'\n",
    "                except:\n",
    "                    continue\n",
    "                # flattening the objects list to make the dimension 1D\n",
    "                objects = flatten(objects)\n",
    "                # terminating leading and trailing spaces from list items\n",
    "                objects = [item.strip() for item in objects]\n",
    "            else:\n",
    "                objects = []\n",
    "        # padding the objects list, we set the limit to 6 objects\n",
    "        objects += ['No Object'] * (6 - len(objects))\n",
    "        if len(objects) > 6:\n",
    "            objects = objects[:6]\n",
    "        temp_lst.append(objects)\n",
    "        posts_lst.append(flatten(temp_lst))\n",
    "\n",
    "    # creating temporary dataframe for posts of this account\n",
    "    temp_df = pd.DataFrame()\n",
    "    temp_df = pd.DataFrame(posts_lst, columns=[\n",
    "        'shortcode',\n",
    "        'post_type',\n",
    "        'username',\n",
    "        'like',\n",
    "        'comment',\n",
    "        'object_1',\n",
    "        'object_2',\n",
    "        'object_3',\n",
    "        'object_4',\n",
    "        'object_5',\n",
    "        'object_6'\n",
    "    ])\n",
    "\n",
    "    if temp_df.username.isin(main_posts_df.username)[0]:\n",
    "        print('User post information already exist, skiping...')\n",
    "        continue\n",
    "    else:\n",
    "        print(f'Adding {username} posts information...')\n",
    "        main_posts_df = pd.concat([main_posts_df, temp_df], axis=0, join='outer')\n",
    "    \n",
    "    # saving the data each time\n",
    "    main_accounts_df.to_csv('Data/accounts.csv')\n",
    "    main_posts_df.to_csv('Data/posts.csv')\n",
    "\n",
    "    # waiting 5 sec for each  user, instagram rate limit\n",
    "    print('Waiting 5 seconds...')\n",
    "    time.sleep(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In the next stage of the CRISP-DM methodology, we have to clean our data for the training phase. Please have in mind that since the insight generation is not part of the competition, we will not undergo an EDA analysis, but an EDA analysis is highly suggested at this stage for any kind of endeavor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_accounts_df = pd.read_csv('Data/accounts.csv')\n",
    "main_posts_df = pd.read_csv('Data/posts.csv')\n",
    "main_accounts_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "main_posts_df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's process the accounts dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values for each feature:\n",
      "id                    0\n",
      "username              0\n",
      "category_name        19\n",
      "follower              0\n",
      "following             0\n",
      "ar_effect             0\n",
      "type_business         0\n",
      "type_professional     0\n",
      "verified              0\n",
      "reel_count            0\n",
      "reel_avg_view         0\n",
      "reel_avg_comment      0\n",
      "reel_avg_like         0\n",
      "reel_avg_duration     0\n",
      "reel_frequency        0\n",
      "media_count           0\n",
      "media_avg_comment     0\n",
      "media_avg_like        0\n",
      "media_frequency       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Number of missing values for each feature:')\n",
    "print(f'{main_accounts_df.isna().sum()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the cell below, the only feature that has missing value is **Category**. We will replace those missing values with \"Unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_accounts_df['category_name'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another data cleaning task that we must do to increase the accuracy and generalizability, is to process the categorical variables. Since we have a good chunk of categorical features in this dataset, we must do this task with careful consideration. There is always a debate regarding the type of encoding the categorical variables, should we use One Hot Encoding (OHE) or Label Encoding (LE). The rule of thumb for this debate rests in cardinality. If the cardinality of the feature is high, we must use label encoding, but if the cardinality is low, we should use label encoding. Let's Explore the cardinality of categorical features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cardinality of category_name:\t\t 42\n",
      "Cardinality of ar_effect:\t\t 2\n",
      "Cardinality of type_business:\t\t 2\n",
      "Cardinality of type_professional:\t 2\n",
      "Cardinality of verified:\t\t 2\n"
     ]
    }
   ],
   "source": [
    "print(f'Cardinality of category_name:\\t\\t {len(main_accounts_df[\"category_name\"].unique())}')\n",
    "print(f'Cardinality of ar_effect:\\t\\t {len(main_accounts_df[\"ar_effect\"].unique())}')\n",
    "print(f'Cardinality of type_business:\\t\\t {len(main_accounts_df[\"type_business\"].unique())}')\n",
    "print(f'Cardinality of type_professional:\\t {len(main_accounts_df[\"type_professional\"].unique())}')\n",
    "print(f'Cardinality of verified:\\t\\t {len(main_accounts_df[\"verified\"].unique())}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the cell above, the only feature with high cardinality is **category_name** and other features are binary categorical features, thus have the low cardinality.\n",
    "\n",
    "***But***, at the time of writing this code, **XGBoost 1.7** had been published, since this version of XGBoost, it can works with categorical variables without the need of manual encoding, thus we won't encode the categorical variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will use the XGBoost and tree-based models for this competetition, feature normalization won't improve the model, thus we will skip the normalization.\n",
    "\n",
    "Now we can process the posts dataframe:\n",
    "\n",
    "First thing we can remove `GraphVideo` type of posts from the dataset since reels on the Instagram don't have detected objects since they are videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_posts_df = main_posts_df.drop(main_posts_df[main_posts_df['post_type'] == 'GraphVideo'].index)\n",
    "main_posts_df = main_posts_df.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after all the cleaning, I can make the main dataset for training the model. To create the main dataset, we must add the account information which is present in the **main_accounts_df** to each corresponding record in **main_posts_df**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       instagram\n",
       "1                       cristiano\n",
       "2                        leomessi\n",
       "3                     selenagomez\n",
       "4                     kyliejenner\n",
       "                  ...            \n",
       "189                   corinthians\n",
       "190                       jokezar\n",
       "191                 ultimahoracol\n",
       "192                sosmedmakassar\n",
       "193    indonesiabertauhidofficial\n",
       "Name: username, Length: 194, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_accounts_df['username']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = main_posts_df.merge(main_accounts_df, on='username')\n",
    "df.to_csv('Data/main_dataset.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of these endeavors, we can train the model. To make the workspace more clear, we will train the model explore it, and visualize it in another notebook.\n",
    "\n",
    "----\n",
    "`@Ramin F.` | [Email](ferdos.ramin@gmail.com) | [LinkedIn](https://www.linkedin.com/in/raminferdos/) | [GitHub](https://github.com/SimplyRamin) | [Personal Portfolio](https://simplyramin.github.io/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
